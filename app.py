# streamlit_app.py
import streamlit as st
from streamlit_drawable_canvas import st_canvas
import torch
import numpy as np
import cv2
from PIL import Image
from model import CRNN, VOCAB, idx2char, ctc_greedy_decode
from text_segmentation import segment_text
from torchvision import transforms
import editdistance
import matplotlib.pyplot as plt
import io
import tempfile

st.title("üíª Hand-CRNN | Recognize Rus Text")

st.sidebar.title("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–∏")
model_name = st.sidebar.selectbox(
    "–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å",
    ("resnet-word-trained-ver3", "resnet-word-trained-ver2")
)

@st.cache_resource
def load_model(name: str):
    model_path = f"model/{name}.pth"
    try:
        state = torch.load(model_path, map_location="cpu")
    except FileNotFoundError:
        st.error(f"–§–∞–π–ª –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {model_path}")
        raise
    model = CRNN(num_classes=len(VOCAB))
    model.load_state_dict(state)
    model.eval()
    return model

# –ó–¥–µ—Å—å –≤—ã–∑—ã–≤–∞–µ–º —Å –∞—Ä–≥—É–º–µ–Ω—Ç–æ–º:
model = load_model(model_name)
st.sidebar.write(f"–ó–∞–≥—Ä—É–∂–µ–Ω–∞ –º–æ–¥–µ–ª—å:")
st.sidebar.write(f"**{model_name}**")

transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

def predict_image(model, img_pil):
    img_tensor = transform(img_pil).unsqueeze(0)  # [1, 1, H, W]
    with torch.no_grad():
        logits = model(img_tensor)
        pred_idxs = torch.argmax(logits, dim=-1).permute(1, 0).tolist()
        decoded = ctc_greedy_decode(pred_idxs, idx2char)
    return decoded[0]

def process_image_from_path(image_path, visualize=False):
    """
    –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Å–ª–æ–≤. 
    –ï—Å–ª–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –Ω–µ –Ω–∞—à–ª–∞ –Ω–∏ –æ–¥–Ω–æ–≥–æ —Å–ª–æ–≤–∞, —Ä–∞—Å–ø–æ–∑–Ω–∞—ë–º –≤—Å—ë –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ü–µ–ª–∏–∫–æ–º.
    """
    # –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è
    original, word_boxes = segment_text(image_path)
    # –ü—Ä–æ—á–∏—Ç–∞–µ–º grayscale-–æ–±—Ä–∞–∑ –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è
    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    results = []

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏: –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ü–≤–µ—Ç–Ω–æ–π, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ —Ä–∏—Å–æ–≤–∞—Ç—å –±–æ–∫—Å—ã
    vis = cv2.cvtColor(image.copy(), cv2.COLOR_GRAY2BGR) if visualize else None

    # –∏–º–µ—é—Ç—Å—è –ª–∏ –≤–æ–æ–±—â–µ –∫–∞–∫–∏–µ-–ª–∏–±–æ –±–æ–∫—Å—ã —Å–ª–æ–≤:
    has_any_box = False
    for line in word_boxes:
        if line:  # –Ω–µ–ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫
            has_any_box = True
            break
    if not has_any_box:
        # –ù–µ—Ç —Å–µ–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–ª–æ–≤ ‚Äî –¥–µ–ª–∞–µ–º –µ–¥–∏–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ –≤—Å—ë –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º original (np.ndarray) –≤ PIL
        pil_full = Image.fromarray(image).convert("L")
        pred_full = predict_image(model, pil_full)
        if visualize:
            st.warning("–°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –Ω–µ –Ω–∞—à–ª–∞ –æ–±–ª–∞—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞. –í—ã–ø–æ–ª–Ω—è–µ–º —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –≤—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.")
            # –ü–æ–∫–∞–∑–∞—Ç—å —Å–∞–º–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –±–µ–∑ –±–æ–∫—Å–æ–≤, —á—Ç–æ–±—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø–æ–Ω—è–ª –ø—Ä–∏—á–∏–Ω—É
            fig, ax = plt.subplots(figsize=(8, 6))
            # –µ—Å–ª–∏ –æ—Ä–∏–≥–∏–Ω–∞–ª —Ü–≤–µ—Ç–Ω–æ–π, –º–æ–∂–Ω–æ –ø–æ–∫–∞–∑–∞—Ç—å –∫–∞–∫ –µ—Å—Ç—å, –Ω–æ —É –Ω–∞—Å grayscale:
            ax.imshow(image, cmap='gray')
            ax.axis("off")
            st.pyplot(fig)
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –µ–¥–∏–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        return pred_full


    # –û–±—ã—á–Ω–∞—è –ª–æ–≥–∏–∫–∞: –ø—Ä–æ–≥–æ–Ω—è–µ–º –ø–æ –Ω–∞–π–¥–µ–Ω–Ω—ã–º —Å–ª–æ–≤–∞–º
    for line in word_boxes:
        for (x, y, w, h) in line:
            word_img = image[y:y+h, x:x+w]
            word_pil = Image.fromarray(word_img).convert("L")
            pred = predict_image(model, word_pil)
            results.append(pred)

            if visualize:
                cv2.rectangle(vis, (x, y), (x + w, y + h), (0, 0, 255), 2)

    if visualize:
        vis_rgb = cv2.cvtColor(vis, cv2.COLOR_BGR2RGB)
        fig, ax = plt.subplots(figsize=(12, 8))
        ax.imshow(vis_rgb)
        ax.axis("off")
        st.pyplot(fig)

    return " ".join(results)

st.title("–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä—É–∫–æ–ø–∏—Å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞")
st.write("*(–Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ)*")

option = st.radio("–í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º –≤–≤–æ–¥–∞:", ["–ù–∞—Ä–∏—Å–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç", "–ó–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ"])

if option == "–ù–∞—Ä–∏—Å–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç":
    canvas_result = st_canvas(
        fill_color="white",
        stroke_width=3,
        stroke_color="black",
        background_color="white",
        height=150,
        width=500,
        drawing_mode="freedraw",
        key="canvas_key",
    )
    if canvas_result.image_data is not None:
        img_data = canvas_result.image_data[:, :, 0].astype(np.uint8)
        img_pil = Image.fromarray(img_data).convert("L")

        with tempfile.NamedTemporaryFile(suffix=".png", delete=False) as tmp_file:
            tmp_path = tmp_file.name
            img_pil.save(tmp_path)

        show_boxes = st.checkbox("–ü–æ–∫–∞–∑–∞—Ç—å –±–æ–∫—Å—ã —Å–ª–æ–≤ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏")

        if st.button("–†–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ç–µ–∫—Å—Ç"):
            st.session_state.canvas_prediction = process_image_from_path(tmp_path, visualize=show_boxes)

        if "canvas_prediction" in st.session_state:
            prediction = st.session_state.canvas_prediction
            st.success(f"–†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: {prediction}")
            gt_text = st.text_input("–í–≤–µ–¥–∏—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç (–¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞)", key="gt_canvas")
            if gt_text:
                cer = editdistance.eval(prediction, gt_text) / max(len(gt_text), 1)
                st.metric("CER (–æ—à–∏–±–∫–∞ –ø–æ —Å–∏–º–≤–æ–ª–∞–º)", f"{cer:.4f}")

elif option == "–ó–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ":
    uploaded_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ PNG/JPEG", type=["png", "jpg", "jpeg"])
    show_boxes = st.checkbox("–ü–æ–∫–∞–∑–∞—Ç—å –±–æ–∫—Å—ã —Å–ª–æ–≤ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏")

    if uploaded_file is not None:
        st.image(uploaded_file, caption="–ó–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ", use_container_width=True)

        with tempfile.NamedTemporaryFile(suffix=".png", delete=False) as tmp_file:
            tmp_file.write(uploaded_file.read())
            tmp_path = tmp_file.name

        if st.button("–†–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏"):
            st.session_state.image_prediction = process_image_from_path(tmp_path, visualize=show_boxes)

        if "image_prediction" in st.session_state:
            final_text = st.session_state.image_prediction
            st.success(f"–†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: {final_text}")
            gt_text = st.text_input("–í–≤–µ–¥–∏—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç (–¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞)", key="gt_image")
            if gt_text:
                cer = editdistance.eval(final_text, gt_text) / max(len(gt_text), 1)
                st.metric("CER (–æ—à–∏–±–∫–∞ –ø–æ —Å–∏–º–≤–æ–ª–∞–º)", f"{cer:.4f}")
